\section{Evaluation}
\label{sec:eval}

\subsection{kfsd Configuration}
\label{sec:eval:configuration}

\newcommand{\kfsgraphscale}{0.5}

Figures~\ref{fig:kfsgraph-journal}, \ref{fig:kfsgraph-su-on-su}, and
\ref{fig:kfsgraph-journal-on-su} show three example kfsd configurations. They
are also example's of kfsgraph's dot output. Figure~\ref{fig:kfsgraph-journal}
shows ``/'' mounted as an internally journaled filesystem using the first IDE
disk. Figure~\ref{fig:kfsgraph-su-on-su} shows ``/loop'' using loop device and
soft updates on top of ``/''. Figure~\ref{fig:kfsgraph-journal-on-su} shows
journaling on soft updates.

\begin{figure}[htb]
\begin{center}
  \includegraphics[scale=\kfsgraphscale]{kfsgraph_journal}
  \caption{kfsd Modules: Journaling}
  \label{fig:kfsgraph-journal}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
  \includegraphics[scale=\kfsgraphscale]{kfsgraph_su_on_su}
  \caption{kfsd Modules: Soft Updates on Soft Updates}
  \label{fig:kfsgraph-su-on-su}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
  \includegraphics[scale=\kfsgraphscale]{kfsgraph_journal_on_su}
  \caption{kfsd Modules: Journaling on Soft Updates}
  \label{fig:kfsgraph-journal-on-su}
\end{center}
\end{figure}

An example use of the mount program is to mount a network block device to
``/net'' with external journaling and a 1024 block write back cache, and to
fsck the filesystem after enabling journaling:\\
\indent \texttt{\$ mount -d nbd 192.168.2.1 -m /net -j /.journal -fsck on -\$ 1024 -wb}\\
Figure~\ref{fig:kfsgraph-mount} depicts kfsd's configuration before and after
this mount.

\begin{figure}[htb]
\begin{center}
  \includegraphics[scale=\kfsgraphscale]{kfsgraph_mount}
  \caption{kfsd Modules: NBD Mount with External Journaling}
  \label{fig:kfsgraph-mount}
\end{center}
\end{figure}


\subsection{Performance}
\label{sec:eval:performance}

Since one motivating factor for this system is to be able to easily tune
filesystems for the way they will be used, it is obviously important to evaluate
the performance of the system. Comparisons to several other filesystem
implementations could be made here: the original JOS file server, Linux running
on the same hardware, and Linux using PIO disk access but using the same
hardware all seem relevant.

Compared to the original JOS file server, the KudOS file server is generally
slower except in two specific ways. First, it can open files much more quickly
than the original file server. This is because the original file server had to
read the entire file off the disk before it could be opened, due to the way all
file read and write operations used shared memory. Second, the KudOS file server
can delete files much more quickly than the original file server, and for the
same reason. (Note that earlier versions of the KudOS file server had the same
issue, but it was easily fixed. This seems to indicate that the new module
structure is reasonable, because it isolated the cause of the inefficiency and
made it easy to change.)

In most other ways, however, the KudOS file server is currently slower than the
original JOS file server - and with good reason: it's doing a whole lot more.
The original file server followed the exokernel spirit a little more closely,
and mapped entire files into client environments when they were opened. Thus all
file I/O could take place without actually communicating with the file server,
as reads and writes could be done to shared memory. This notable absense of
explicit communication with the file server for the most common operations on
files greatly sped up the original file server most of the time.

Compared to Linux, the KudOS file server does not yet seem competitive. We
believe most of this speed discrepancy is due to the fact that our caches
currently only write blocks when they are evicted out of necessity, which
obviously is a poor scheduling policy. We have a mechanism to solve this
problem, and the journal module uses it for its own scheduling, but it has not
yet been used for the caches. We will save numerical performance testing until
the KudOS file server's caches are more like other caches in these respects.

\subsection{Simplicity}
\label{sec:eval:simplicity}

rules are hard to follow.
