\section{Evaluation}
\label{sec:eval}

\subsection{kfsd Configuration}
\label{sec:eval:configuration}

\newcommand{\kfsgraphscale}{0.5}

Figures~\ref{fig:kfsgraph-journal}, \ref{fig:kfsgraph-su-on-su}, and
\ref{fig:kfsgraph-journal-on-su} show three example kfsd configurations. They
are also examples of kfsgraph's dot output. Figure~\ref{fig:kfsgraph-journal}
shows ``/'' mounted as an internally journaled filesystem using the first IDE
disk. Figure~\ref{fig:kfsgraph-su-on-su} shows ``/loop'' using loop device and
soft updates on top of ``/''. Figure~\ref{fig:kfsgraph-journal-on-su} shows
journaling on soft updates.

\begin{figure}[htb]
\begin{center}
  \includegraphics[scale=\kfsgraphscale]{kfsgraph_journal}
  \caption{kfsd Modules: Journaling}
  \label{fig:kfsgraph-journal}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
  \includegraphics[scale=\kfsgraphscale]{kfsgraph_su_on_su}
  \caption{kfsd Modules: Soft Updates on Soft Updates}
  \label{fig:kfsgraph-su-on-su}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
  \includegraphics[scale=\kfsgraphscale]{kfsgraph_journal_on_su}
  \caption{kfsd Modules: Journaling on Soft Updates}
  \label{fig:kfsgraph-journal-on-su}
\end{center}
\end{figure}

An example use of the mount program is to mount a network block device to
``/net'' with external journaling and a 1024 block write back cache, and to
fsck the filesystem after enabling journaling:\\
\indent \texttt{\$ mount -d nbd 192.168.2.1 -m /net -j /.journal -fsck on -\$ 1024 -wb}\\
Figure~\ref{fig:kfsgraph-mount} depicts kfsd's configuration before and after
this mount.

\begin{figure}[htb]
\begin{center}
  \includegraphics[scale=\kfsgraphscale]{kfsgraph_mount}
  \caption{kfsd Modules: NBD Mount with External Journaling}
  \label{fig:kfsgraph-mount}
\end{center}
\end{figure}

\subsection{Simplicity}
\label{sec:eval:simplicity}

One motivating factor for this system was to make understanding existing code
and creating new code easier. The ease with which the network block device,
RAID, and even the journal modules were written is a testament to the success of
this part of the project. The larger modules, like josfs\_base and wb\_cache,
are not quite as easy to read, but still they are not intractable. Further, the
corresponding code in existing systems is not so well isolated, and makes
reading other code harder due to its presence there.

However, there is one problem in the system that makes writing new modules a
little bit more difficult than it might otherwise be. The two major structures
we use, bdescs and chdescs, both have usage conventions that the programmer is
required to follow in order to make reference counting work correctly. Violating
these rules can cause hidden bugs that happen rarely, but cause anywhere from
data corruption to a file server crash when they do. Failing to follow the
conventions as they evolved during the development of the system was a major
source of bugs, and it would likely cause problems for other module
implementors. A system to automatically generate modules that follow the
conventions from higher-level descriptions could perhaps alleviate this problem,
similar to the system in \cite{zadok00fist}.

Another reason to create the KudOS file server was to be able to easily tune
filesystems for the way they will be used, and to be able to look at a file
server configuration and understand how it will behave (or how to change its
behavior to match how it will be used). This goal has been met quite well by the
variety of modules we have created, the flexibility of the interfaces to add new
modules, and the kfsgraph tool for graphically viewing the configuration of a
running KudOS file server.

\subsection{Performance}
\label{sec:eval:performance}

Tuning a file server configuration to make it handle its expected work optimally
is all well and good, but if the file server's maximum performance is not so
good in the first place, it is unlikely to be immediately useful. It is
therefore important to evaluate the performance of the system. Comparisons to
several other filesystem implementations could be made here: the original JOS
file server, Linux running on the same hardware, and Linux using PIO disk access
but using the same hardware all seem relevant.

Compared to the original JOS file server, the KudOS file server is generally
slower except in two specific ways. First, it can open files much more quickly
than the original file server. This is because the original file server had to
read the entire file off the disk before it could be opened, due to the way all
file read and write operations used shared memory. Second, the KudOS file server
can delete files much more quickly than the original file server, and for the
same reason. (Note that earlier versions of the KudOS file server had the same
issue, but it was easily fixed. This seems to indicate that the new module
structure is reasonable, because it isolated the cause of the inefficiency and
made it easy to change.)

In most other ways, however, the KudOS file server is currently slower than the
original JOS file server - and with good reason: it's doing a whole lot more.
The original file server followed the exokernel spirit a little more closely,
and mapped entire files into client environments when they were opened. Thus all
file I/O could take place without actually communicating with the file server,
as reads and writes could be done to shared memory. This notable absense of
explicit communication with the file server for the most common operations on
files greatly sped up the original file server most of the time.

Compared to Linux, the KudOS file server does not yet seem competitive. We
believe most of this speed discrepancy is due to the fact that our caches
currently only write blocks when they are evicted out of necessity, which
obviously is a poor scheduling policy. We have a mechanism to solve this
problem, and the journal module uses it for its own scheduling, but it has not
yet been used for the caches. We will save numerical performance testing until
the KudOS file server's caches are more like other caches in these respects.
