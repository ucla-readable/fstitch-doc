\subsection{Implementation}
\label{sec:solution:impl}

(talk about how we did some speed optimizations already somewhere in this file)

intro paragraph, we implemented these things... blah blah

\subsubsection{KPL}

must integrate user level file descriptor interface. (why this is
warranted). provides capabilities page.

\subsubsection{UHFS}
\label{sec:solution:impl:uhfs}

If the CFS layer is akin to C and the LFS layer to assembly, the UHFS (Uniform
High level File System) module is a CFS to LFS interpreter. UHFS is intended to
be the common CFS-LFS module for all file systems with an LFS interface; this
includes josfs\_base and does not include devfs\_cfs. UHFS's CFS function
implementations take care of non-block aligned accesses, lower level operations
such as block allocation during write, and orders inter-LFS function call chdesc
dependencies.

uhfs\_write() provides an example of a typical UHFS function. UHFS's client
passes an open file's fid, data to write, and where in the file this data
belongs. uhfs\_write() looks up the fid's associated LFS fdesc and if the below
LFS supports file sizes, looks up the file's current size in bytes.
uhfs\_write() then writes the data a block at a time to the below LFS,
allocating and then appending blocks as the file needs enlargement. Once all
data is written and if the below LFS supports the file size feature,
uhfs\_write() updates the file size metadata.

\subsubsection{josfs\_base}

in principle, only module in the system which needs to know about hwo
the bytes are arranged on the disk. in fact, that's all it knows. also
must order writes, generate change descriptors, possible chain them
up. this module implements the low level operations that UHFS requests
(e.g. allocate a block).

\subsubsection{josfs\_cfs Legacy Module}

In the beginning of the KudOS file server's development we wanted to begin
testing CFS modules, CFS RPC, and KPL before the LFS and BD layers were ready
for use. The josfs\_cfs module allowed this testing by providing a full file
system, using JOS's existing file server daemon as a back-end. josfs\_cfs's
simplicity also served as a first validation that the CFS interface is flexible
enough to host new and helpful uses.

\subsubsection{Reliable File System Features}

soft updates is more pervasive. because of this, we cna't just have a
moduel to do it. we need clues (a must happen beofre b). josfs base
and uhfs cooperate to give us the hints/clues (actually dependencies)
to hookup the chdecs into a graph that would comply w/ soft
updates. (wb cache make sure this is honored).

diagram that shows journal format. full data only. uses Q to intercept
things. unlike ext3 we have a bandwidth limitation which we could (?)
work around. very similar to ext3. fs doesn't know ti's being
journaled. it's a xaction layer on top of BD. guaranatess sets fo
writes are atomic. each high level cfs op is atomic now. maybe sets
clumped together, but a cfs call is never broken apart.

\subsubsection{wb\_cache}

makes sure that dep graph is satisifed

direct mapped cache. talk about algo used to evict blocks. the algo is
not optimal. in practice it is optimal, but in practice we don't have
very complex graphs. produces ooutput that's serialized and safe for
writing to stable storage.

design philosophy: can't stack the wb cache. it has to know that
writes go to disk in the correct order. sync() to make sure stuff goes
to disk at the correct order.

sure, stacking wb cache seems cool, but why not just put a wb cache
above a disk and call it a day? floating deps down gives no benefit.

you could probably do a wb cache that doesn't honor deps but depends
on this wb cache to honor them. but aht's a problem sucking blocks
down.

\subsubsection{RAID}
\label{sec:solution:impl:raid}

lets you strip or mirror reads/writes to 1 or 2 disks. mirroring
driver can handle disk failure and go into degraded mode where it's a
pass thru. also mirroring can do what geom did where you hot swap in a
new disk, syn, ditch the old disk (w/ help of modman and userland
utilities).

\subsubsection{Loop Device}
\label{sec:solution:impl:loop}

The loop device provides a BD interface to an LFS file. Through change
descriptors and the LFS interface, the loop device is able to easily
pass change descriptors to the underlying filesystem, preserving
softupdates and journaling dependencies. To write or sync a block, the
loop device sets the block's BD pointer and block number for use by
the LFS's underlying BD, makes the appropriate LFS call, reverts the
block's changes, and returns.

\subsubsection{Network Block Device}

The network block device is extremely simple. It uses a straightforward
serialization of the BD interface over a TCP connection. During initialization,
it receives the block size and number of blocks from the server. For each read
request, it sends a read command and a block number to the server, and waits for
the block to be returned. For a write request, it sends a write command, block
number, and the block data. Both the client and server (which can run on either
a POSIX system or KudOS itself) required only a few hours to develop and test,
and they fit right into the rest of the system like any other block device.

\subsubsection{Online Configuration}
\label{sec:solution:impl:online}

The modman component is central to most online configuration and introspection,
providing existence, usage, configuration, and status information for CFS, LFS,
and BD module instances. Each module instance registers/unregisters itself with
modman at creation/destruction time and registers/unregisters module instance
usage. modman stores this information to respond to others' queries.

KFS RPC is implemented in a similar manner to CFS RPC: a serialized KFS is used
to communicate between KFS server and clients via KudOS IPC message passing.
Each KFS function is re-implemented as an RPC stub, allowing an object file to
be linked to either the KFS IPC Client or Server library. Client programs are
thus able to reconfigure the kfsd environment using the same functions that
would be used within kfsd.

The program kfsgraph uses KFS RPC to construct a module usage graph, showing
module instance existence, which instances use which others, and instances'
configuration and status. kfsgraph can output this graph to text or to AT\&T's
graphviz dot format. Section~\ref{sec:eval} shows examples of kfsgraph's output.

The programs mount and umount, named for their similarity to the Unix tools of
the same names, provide a general case, easy to use configuration interface.
Given a BD type of IDE, nbd, loop, or an existing BD and a mount point, mount
constructs the given base BD, caches and block resizers as appropriate, a
josfs\_base or wholedisk as appropriate, UHFS, and connects these to the table
classifier at the given mount point. mount optionally allows specification of
whether journaling is to be used or not (and where to journal to), whether to
fsck the filesystem, how large a cache to use, and whether to use a write back
or write through cache. umount takes a mount point and destructs all modules
down the chain that are no longer in use.
