> ==+== sosp2007 Paper Reviews
> 
> ==+== =====================================================================
> ==+== Begin Review #169A
> ==-== Paper: Generalized File System Dependencies
> ==-== Updated Monday 23 Apr 2007 1:49:25pm EDT
> 
> ==+== A. Overall merit
> 
> 3. Weak accept
> 
> ==+== B. Novelty
> 
> 4. Novel
> 
> ==+== C. Writing
> 
> 4. Good
> 
> ==+== D. Reviewer expertise
> 
> 2. Some familiarity
> 
> ==+== E. Paper summary
> 
> The paper proposes a generalized abstraction, called "patches", for
> describing ordering constrainst among modifications to stable storage,
> and shows how it can be used in file systems to replace
> system-specific implementation for ensuring consistency in the
> presence of failures. Patches directly describe the dependencies
> between writes of disk blocks (in the file system context). They can
> be used to implement soft updates, journaling, and other consistency
> models that typically have special purpose implementations. A naive
> implementation of patches has very high overhead in terms of CPU and
> memory usage. The authors describe several optimizations to patches
> that improves performance significantly and puts the idea in the realm
> of the feasible. Their performance results show that there is still a
> ways to go to get patches to perform as well as existing Linux systems
> that use special-case consistency implementations.
> 
> ==+== F. Comments for author
> 
> In general I like the paper. I think the idea is interesting, and you
> give a balanced presentation of its current pros and cons. This seems
> like an attempt at generalization that is may be worthwhile (unlike
> so many others). If you can achieve the performance improvements that
> you are aiming for it sounds like this could become a practical
> abstraction for implementing stable storage systems.

{Performance}

We have since improved Dodder performance for the tar and rm
benchmarks and now surpass Linux ext3 (which provides similar
consistency guarantees) for elapsed real time. Much of Dodder's time
improvement since the paper's submission has come from improving
things like the block allocation strategy! However, the point of soft
updates is for it to perform almost as fast, or even faster than,
asynchronous ordering, and Dodder ext2 with soft updates still trails
Linux ext2 performance for the tar and postmark benchmarks.

Currently for tar, Dodder ext2 with soft updates takes 9.3s real time,
Linux ext2 6.0s, and Linux ext3 13.8s. For rm, Dodder ext2 with soft
updates takes 1.8s, Linux ext2 4.9s, and Linux ext3 4.7s.

We have also further decreased patch memory usage. Dodder now creates
74,000 and 1,800 patches for the tar and rm benchmarks, compared to
276,000 and 54,000 for paper submission and 537,000 and 193,000
without optimizations. Patches now create 353 kB and 0 B of rollback
data for the tar and rm benchmarks, compared to 1,269 kB and 655 kB for
paper submission and 469,647 kB and 3,246 kB without optimizations.

However, Dodder still does not match Linux ext2/3 in other areas.
Dodder uses up to 4.8 times the cpu time that Linux ext2/3 use for the
tar and rm benchmark, and uses 20 times the real time (8.6s vs 0.4s)
for a small Postmark run. Dodder writes 93,558 kB for a small Postmark
run that fits in memory, whereas Linux ext2 writes 96 kB and Linux
ext3 217 kB. Whereas Dodder writes all dirtied blocks for Postmark,
Linux ext2 and Linux ext3 write only end result dirty blocks.

We are actively working to improve Dodder's performance in the
conditions Postmark exposes. First, we believe the primary reason that
large Postmark runs (those that do not fit in memory) are slower than
Linux is poor block allocation. We have partially addressed this
already, as can be seen by the tar runtime decrease since submission,
and plan to further improve block allocation to avoid reuse of
recently freed blocks (which will reduce the number of block-level
cycles and thus sectors written multiple times). We also hope to add
an optimization to Dodder ext2 and patches to detect, and then not
write, dirty (but now unreferenced) blocks. Though we believe this is
only a small issue for large Postmark runs, since the amount of writes
saved will be small compared to the number of necessary writes.

> 
> Here are some specific comments:
> 
> You use the phrase "soft updates-like consistency" several
> times. Aside from being a bit awkward, you don't define what it means
> until well after the first use. I think I first noticed it on page 3
> ("Rollback data" subsection).

The phrase does feel awkward. We will try to improve this.
The phrase "soft updates-like consistency" is actually not used until
the "Rollback data" subsection. This issue is part of our need to
generally better explain soft updates. See {Soft Updates Explanation}.

> 
> In the Rollback data section, I found the discussion about circular
> dependencies a little hard to follow. May an example would help?

{Soft Updates Explanation}

All of the reviewers noted that they would like a more in depth and/or
more clear explanation of soft updates, block-level dependency cycles,
and change rollback. We plan to address this.

We are concerned, however, about space. We may take advantage of any
additional pages available, but would like your suggestions for parts
of the paper to cut, if any.
 
> 
> In Section 6, at the top of page 9, you write "When the function is
> called, the patch *p is set to the patch, if any, on which the
> modification should depend; when the function returns, the patch *p
> must be set to depend on the modification itself." Did you mean that
> *p must be set to the patch corresponding to the modification? (I
> don't understand what it means otherwise.)

We plan to clarify this patch interface convention explanation.

> 
> In section 7.1 on the patchgroups interface, you describe restrictions
> to prevent cycles. So, can you create patchgroups p and q, then issue
> pg_depend(q,p), and then engage each of p and q? (Is the rule just
> that they can't be engaged before the "depend" call?) I found that a
> little confusing on first read.

{Patchgroup Interface}

Several reviewers would like more information on how patchgroups are
used, work, and perform. We plan to expand our patchgroup explanations
and address these points.

> 
> Can patches in a patchgroup be sent to disk before a patchgroup is
> closed? Does closing a patchgroup do anything special?

See {Patchgroup Interface}. We plan to clarify this point.

> 
> In the description of using patchgroups in Subversion, I think Figure
> 8 would be more useful if you could tag the various components and
> dependencies in the figure and refer to them in the discussion.

We will either try to clarify this point or we may drop the figure, in the
case that we decide it does not add much to the subversion case study.

> 
> Finally, in Section 8, in describing the implementation, it would help
> readers who are not Linux kernel experts (like me) to understand better how
> Dodder fits in if you could give a little more context about how the
> Linux kernel is structured around the buffer cache and file system
> layers.

We will add more information on how the Linux kernel is structured
and on how Dodder fits into Linux.

> 
> ==+== End Review
> 
> ==+== =====================================================================
> ==+== Begin Review #169B
> ==-== Paper: Generalized File System Dependencies
> ==-== Updated Monday 23 Apr 2007 2:15:16pm EDT
> 
> ==+== A. Overall merit
> 
> 4. Accept
> 
> ==+== B. Novelty
> 
> 4. Novel
> 
> ==+== C. Writing
> 
> 4. Good
> 
> ==+== D. Reviewer expertise
> 
> 4. Expert
> 
> ==+== E. Paper summary
> 
> This paper describes a general abstraction, patches, for representing
> write ordering in file systems.  The motivation is that by explicitly
> labeling dependencies, one can be sure that updates are performed
> consistently.  The authors describe a number of essential
> optimizations for reducing the number of patches and rollbacks.  The
> authors implement soft updates, journaling, and asynchronous writes
> (no dependencies) with Dodder (though they do not show performance for
> journaling). The authors also describe how applications can use
> patchgroups to specify dependencies.
> 
> I like this paper and think that it makes interesting contributions.
> First, the authors have implemented a general framework in Linux for
> defining write dependencies; this framework is used to specify
> consistency protocols such as soft updates and journaling.  Second,
> the authors describe (and briefly evaluate) optimizations for reducing
> the needed number of patches and rollbacks; these optimizations are
> technically and conceptually interesting and non-trivial.
> 
> The serious problem with this work is that the performance of Dodder
> is many times worse than that of ext2 or ext3.  The authors state that
> their overall performance is "competitive" (abstract) and "within
> reason" (Section 9), but I don't agree.  Their current
> numbers (for Postmark) indicate that Dodder requires 47s, compared to
> 17s and 20s, for ext2 and ext3 respectively; their results for
> microbenchmarks of untar and delete and similarly worrisome.  

See {Performance}. We are addressing Dodder's performance.

> 
> Another weakness is that the authors have not performed a
> thorough evaluation.  The best way to compare performance would be to
> implement soft updates directly in ext2 and with Dodder, and compare
> the two; the authors do not do this.  Additionally, the authors should
> compare ext3 to Dodder with journaling, but they show no numbers for
> Dodder journaling.

We understand the reviewer's interest, and compare Dodder ext2 with
soft updates against FreeBSD UFS with soft updates for this very
reason. However, this comparison is also flawed because of OS
differences, as reviewers point out. While we do not feel that we have
the time to add soft update ordering to Linux ext2, we may be able to
add sufficient support to run just the tar benchmark. We also feel
that this would provide almost as much of a performance insight as
measuring all benchmarks. We would be interested to know how
interesting and useful you feel this intermediate approach would be.

{Evaluation}

We and many of the reviewers feel that the submitted evaluation was
lacking in breadth, depth, and clarity. We plan to significantly expand
and rewrite the section to address these lackings. Specifically, we plan to:
- add Dodder journaled ext2 to the benchmarks;
- show the # dependencies created to Figure 9 (opt effectiveness);
- break down real time to also show system and I/O time for the benchmarks;
- add another standard macro-benchmark;
- improve patchgroup evaluation similarly (see {Patchgroup Evaluation});
- improve writing clarity.

> 
> I am also disappointed with the evaulation of the case studies using
> patchgroups.  The gzip example could be solved with an fsync and the
> subversion example already provides safe commits with syncs on the
> server and journaling on the client; the performance of the non-patch
> versions is in no way compared to patches.  

We see, but somewhat disagree with, this reviewer's view of the gzip
and subversion case studies.

{Patchgroup Evaluation}

The gzip case study goal is to avoid the fsync. A motivating scenario:
if the extracted file is soon moved to another file system or deleted
the extracted file data written to disk will increase the time to
gunzip unnecessarily and possibly significantly. This case study also
serves as a conceptually simple patchgroup example.

The subversion working copy case study goal is to decrease the block
write ordering strictness of svn wc to improve performance, make
svn wc safe against system crashes on systems that do not use as
strict ordering requirements as Linux ext3 (e.g., NTFS), and to show
how a larger piece of software already concerned with write ordering
can be adapted to take advantage of patchgroups. We did not aim to
rewrite svn wc from the ground up, though, in response to this reviewer's
comments, we believe rewriting svn wc would perhaps increase
the performance and simplify the software.

We plan to also measure time, and perhaps memory usage, to the IMAP
benchmark evaluation. And we plan to also evaluate the performance of
both the gzip and subversion modifications.

> The IMAP example, which originally used syncs, is more compelling.
> This is the only example that the authors show any performance
> results for; however, their metric is # of writes, and not
> performance in time.  It concerns me that the authors are hiding
> poor performance with their indirect metric.  I would like to see
> performance results for all three case studies.

We originally showed #writes to compare the I/O requirements of Dodder and
existing. However, CPU and I/O time requirements are also critical and
we plan to add these measurements to the evaluation.

> 
> My final serious concern is that the motivation for patches is not
> strong.  Is a general abstraction really needed?  Where are the
> benefits?  (Certainly it is not a performance benefit.)  The authors
> mention that patches give file system implementations some flexibility
> in how dependencies are enforced, but they do not explore this
> flexibility - they don't have results with multiple implementations.

{Motivation}

Many of the reviewers felt that the paper's motivation was lacking.
We believe the paper should more strongly emphasize and motivate
patches as an enabling technology for new models --- e.g., patchgroups.
In this paper we investigate using patches to describe all file system
changes and describe the benefits of doing so (largely, patches permit
explicit order specification and enable modular inspection and
transformation of disk changes) as well as the drawbacks (at least,
unoptimized cpu and memory overhead).

We believe we should also more clearly convey the fact that Dodder is
not a refinement of soft updates, but a generalization of the
write-before relationship present in many storage systems. And perhaps
more strongly emphasize Dodder's novel optimization techniques that
makes the use of these generalizations feasible.

> 
> I would like to see this paper accepted, under the assumption that the
> authors are able to improve the performance of Dodder and more
> thoroughly evaluate Dodder.

See {Performance} and {Evaluation}. We plan to strongly improve both of
these aspects.

> 
> ==+== F. Comments for author
> 
> As stated above, I think the most needed improvement for this paper is
> to improve the performance and the evaluation of your system.
> 
> - Related Work: I'd like to see more comparison with Burnett's thesis.  Is Burnett's
>   approach only for applications?  How does his interface differ from
>   patches?

We hope to expand on the relationship between Burnett's thesis and our
work. Our patchgroup interface is very similar to his interface to the
kernel (graphwrite). However, the methods used to implement the
interfaces are very different: his system tracks dependencies at the
level of entire graphwrite() calls, associates dirty blocks with a
unique ID returned by that call, and duplicates dirty blocks when
necessary to preserve ordering. Our work tracks individual changes to
blocks internally, allowing kernel modules a finer level of control, and
only chooses to expose an interface similar to his to userspace as a
means to simplify the sanity checking which is required of arbitrary
user-submitted requests.

Additionally, Burnett did not implement the system described in section
4 of his thesis. Instead it is evaluated using trace-driven simulations.
We have found that some aspects of real disks have significant impacts
which his simulator may not take into account; for instance, the number
of I/O requests to the disk, rather than the number of blocks read or
written by each request, is generally the bottleneck in our tests.

> 
> - Much of the text would be easier to follow if you included more
>   examples (though I understand that you are space constrained right
>   now).  For example, a case illustrating how the file system can
>   create circular dependencies, even when the data itself does not (pg
>   3).  I could have used more explanation for when hard patches cannot
>   be applied; specifically, Figure 2 was useful for showing when the
>   optimizations could be applied, but I could use an example where
>   they cannot be applied too.  It would be useful to see the patches
>   that result from more file and directory operations for soft
>   updates.  For Figure 3, could you provide an example that represents
>   specific updates?

See {Soft Updates Explanation}. We plan to improve our soft updates
explanation. We agree with these comments and plan to implement them
to make the patch optimization techniques more understandable.

> 
> - The authors consider the approach of carrying rollback data as a
>   part of a patch, in case there is a circular dependency.  It seems
>   another approach to breaking the dependency would be to create a
>   copy of the page causing the circular dependency; is this
>   equivalent?  how does this approach compare?

This is an interesting alternative approach, used in Burnett's thesis
(see above), but we believe the common patch and patch dependency case
is better optimized by hard patches and patch merging. Our intuition is
that duplicating pages would increase, not reduce, memory pressure when
using fine-grained patch dependencies. Additionally, since patches can
be rolled back independently, a page with two patches on it can be
rolled back to a state where either patch is present without the other.

> 
> - Figures 4 and 5: I would have preferred a diagram that shows the file
>   system data structures being manipulated (similar to Figure 2)
>   instead.

We plan to make this change.

> 
> - Section 5, Journaling: It took me awhile to figure out what you
>   meant by your last paragraph.  I think the problem is that this
>   discussion assumes knowledge of the Modules, not described until
>   Section 6.  I'd also say that your model is similar to ext3's *data*
>   journaling mode (if that is indeed the case, and not the default
>   ordered journaling mode).

We plan to rewrite much of the journal explanation and address this
section ordering difficulty. The journal module supports full data and
ordered data modes; we will try to clarify this.

> 
> - Figure 6: The font for the interface specifications is too small
>   (CFS, L2FS, BD).  

We will address the small font size, perhaps by removing the figure.

> 
> - I'd be interested in seeing the experiments exploring the
>   sensitivity to the write-back cache policy for dirty blocks.  It
>   certainly seems that performance could be improved by waiting until
>   more patches are specified before flushing.

Out performance investigations have actually reduced our belief that the
write-back cache policy has such a large effect; for instance, we
obtained a very large performance improvement recently by changing the
block allocation policy of our ext2 module to help coalesce block
regions which have similar dependencies and thus can be written
together. We hope to have a much better handle on performance and what
affects it most in the final version.

> 
> - As stated above, I would like to see more detailed analysis of
>   patchgroups.  First, I'd like to see all three applications
>   analyzed.  Second, the metric should be a direct measurement of
>   performance, and not simply of the number of writes.  Third, I'd
>   like an explanation for the performance differences and comments on
>   any difference in ordering guarantees across the versions.

See {Patchgroup Evaluation}. We plan to address all three points.

>
> - You might want to consider how you could express shadow paging, as
>   in WAFL, with patches.  Being able to demonstrate an additional
>   consistency mechnanism would help convince readers of the generality
>   of patches.  I believe this should be straight forward.

We are not sure that this would contribute much to the paper; probably a
better discussion of soft updates, journaling, patches, and patchgroups
will help to demonstrate the flexibility of patches in the way this
reviewer wants. What do you think?

> 
> ==+== End Review
> 
> ==+== =====================================================================
> ==+== Begin Review #169C
> ==-== Paper: Generalized File System Dependencies
> ==-== Updated Tuesday 24 Apr 2007 9:56:48pm EDT
> 
> ==+== A. Overall merit
> 
> 3. Weak accept
> 
> ==+== B. Novelty
> 
> 3. Incremental
> 
> ==+== C. Writing
> 
> 4. Good
> 
> ==+== D. Reviewer expertise
> 
> 2. Some familiarity
> 
> ==+== E. Paper summary
> 
> This paper proposes a general patch abstraction that can capture any
> write dependencies in a file system. The authors demonstrate the
> generality and practicality of their approach by showing how several
> file applications and systems, such as gzip and svn, can be
> implemented on top of the patch abstraction. The paper presents
> several optimizations that dramatically reduce the overhead of their
> patching implementation.
> 
> ==+== F. Comments for author
> 
> This is a solid paper. My comments are below.
> 
> While in general I like the paper, I am not sure whether the decision
> to add the patch functionality at the lower layers of the storage
> stack is the right one. The decision of how much functionality to add
> at a lower layer is often a hard one. In the networking world, this
> decision is typically resolved by invoking the end-to-end arguments
> principle. In one of its interpretations, this principle says that it
> is fine to add functionality at a lower layer as long as this
> functionality can dramatically improve the performance of some
> important applications, and, at the same time, does not negatively
> impact the applications that do not use the new functionality. I am
> not sure that the proposed solution passes this threshold. A more
> comprehensive discussion of this decision would help.

The performance of Dodder has been improved substantially since the
submitted version of the paper, and it is now the case that when all
patch dependencies are disconnected ("not using the new functionality")
Dodder is actually faster than Linux ext2 at the untar and rm tests. (We
have not yet tried Postmark in this mode.) Dodder ext2 in soft updates
mode is also faster than Linux ext3 at these tests. Finally, see
{Performance}, as we believe that the final version of the paper will be
able to show improved performance in our patchgroup case studies
provided by the patchgroup interface and the underlying patches.

> 
> Section 7.2 does a good job of evaluating the solution by showing that
> implementing it requires small changes to existing applications and
> the overhead is "reasonable". One thing that is missing though is
> quantifying the savings due to the better control of dependencies
> enabled by the patch abstraction. In both the case of the subversion
> example and the UW IMAP example, using the patch abstraction allows
> one to relax the dependencies imposed by the original
> application. Intuitively, this should have a positive effect on
> performance. It is true that in the end the overhead of implementing
> patching may offset these gains, but it would be still interesting to
> quantify the potential gains.

See {Patchgroup Evaluation}. We plan to add these patchgroup measurements.

> 
> Other comments and typos:
> 
> - page 1, 2nd column, 3rd para: "optimizations that which" ->
>   "optimizations that"
> 
> - page 4, 2nd column, 4th para: "On the one hand" -> "On one hand"

We agree with the first, but not the second, suggestion.

> 
> - page 4: You say that your implementation "currently disallows any
>   dependency cycles..." What is the impact of this constraint, if any?

See {Patchgroup Interface}. We plan to further explain the impact of the
acyclic requirement.

> 
> - page 5: You should explain better Figure 2(a). It took me quite a
>   bit of time to understand what each patch represents.

We will try to improve these and similar phrases.

> 
> - page 8, "Soft updates": While in the end it is clear from the text
>   what kind of consistency you are referring to, it would be nice to
>   avoid vague formulations such as "relative consistency" and "more
>   serious inconsistencies"

We will try to improve these and similar phrases.

> 
> - page 8, sec 6: "There are has" -> "There are"
> 
> - page 10, 2nd column, para before Sec 7.2: "When an patchgroup" ->
>   "When a patchgroup"
> 
> - page 10, "Gzip": Remove one occurrence of "trivial" from the first
>   sentence.

We plan to make these grammar improvements.

> 
> ==+== End Review
> 
> ==+== =====================================================================
> ==+== Begin Review #169D
> ==-== Paper: Generalized File System Dependencies
> ==-== Updated Monday 14 May 2007 10:24:13pm EDT
> 
> ==+== A. Overall merit
> 
> 2. Weak reject
> 
> ==+== B. Novelty
> 
> 2. Very incremental
> 
> ==+== C. Writing
> 
> 3. Average
> 
> ==+== D. Reviewer expertise
> 
> 3. Knowledgeable
> 
> ==+== E. Paper summary
> 
> the paper argues for explicitly specifying which disk writes depend on
> which others. Specification is done on a fine-grained basis (e.g.,
> each updated field in an inode might be specified as a separate write)
> and the run-time system merges writes to keep things reasonably
> efficient. The runtime system then ensures that a write doesn't make
> it to disk before the blocks on which it depends. 
> 
> Claimed benefits
> 
>   o It is A Good Thing to specify dependencies explicitly
> 
>   o Simplifies constructing file systems
> 
>   o User-level processes can specify dependencies that are correctly
>     propagated through the system's layers
> 
>   o The optimizations keep the overheads of this approach managable
> 
> 
> Limitations
> 
>   o The motivation/significance of this work seems limited

See {Motivation}. We plan to improve the paper's motivation.

> 
>   o This paper primarily cleans up and simplifies ideas from the soft
>     updates paper rather than introducing a fundamentally new approach.

{Novelty}

We disagree with this characterization. We view user controlled
dependencies (patchgroups), the pervasiveness of patches throughout
the system and the optimizations required to efficiently realize this,
and the benefits of patches throughout the system as interesting and novel
advances and of wider scope than soft updates. We hope to better convey
the reason for this view through improving the paper's motivation
(see {Motivation}).

> 
>   o Most of the writing is quite clear, but a few crucial points
>    (regarding cycles and rollback) were not made clear to me even
>    after several careful readings

See {Soft Updates Explanation}. We plan to clarify cycles and rollback.

> 
>   o The performance hit is currently non-trivial, and I'm not sure
>     that it will be an easy sell to get people to pay the cost of
>     significantly worse performance in order to get the benefit of "a
>     cleaner abstraction"

See {Performance}. We are addressing Dodder's performance.

> 
> Summary recommendation
> 
>   Although I half buy the philosophy that explicitly specifying
>   dependencies seems like a good way to ensure dependencies are
>   respected, the practical impact seems marginal. Also, the basic
>   ideas don't seem that different than the soft updates paper (as the
>   title promises, this paper "generalizes" file system dependencies,
>   it doesn't introduce the idea.) Worse, this generalization ends up
>   coming at a pretty high performance cost.  As a result, I find it
>   hard to argue for accepting the paper. My rating of "weak reject"
>   reflects that I have positive feelings about the paper, but that I
>   don't think it is in the top half of the (generally very good to
>   excellent) papers I reviewed.

See above responses to these summary points.
Practical impact: {Motivation}, novelty: {Novelty}, cost: {Performance}.

> 
> ==+== F. Comments for author
> 
> Two high level points then a few detailed comments.
> 
> High level question -- is this the right abstraction? An
> alternative would be to expose a transactional interface.  Sure, you
> lose the ability to switch between "soft updates" and "logging" styles
> of reliability, but (a) I'm not entirely confinced this is a big loss
> and (b) you may end up with simpler specifications of systems above
> this layer. 

We plan to resolve this question through our motivation improvements.
Specifically, the goal the patch abstraction is to express the
write-before relationship used in many storage systems. This is a
different abstraction than transactions.

> 
> As the authors make clear in the title, this paper generalizes ideas
> from the 1999 USENIX soft updates paper. In particular, the basic
> ideas of "track dependencies at fine grained basis" and "roll back to
> fix cycles" were both in the 1999 paper ("The key attribute of soft
> updates is dependency tracking at the level of individual changes
> within cached blocks.... With this level of detailed dependency
> information, circular dependencies between blocks are not problematic.
> For example, when the system wishes to write a buffer containing
> inodes, those inodes that can be safely written can go to the
> disk. Any inodes that cannot be safely written yet are temporarily
> rolled back to their safe values while the disk write proceeds. After
> the disk write completes, such inodes are rolled forward to their
> current values.")
> 
> Refining our understanding of existing ideas is a laudible goal. The
> paper would be stronger if the ideas can be refined to the point where
> the costs of the general system are comparable to hand-tuned
> systems. (That will be even more convincing information that the work
> has a complete "deep" understanding of the issues.)

We believe our motivation improvements will resolve this weakness.
There is some refinement in this work, at the level of motivation perhaps.
But in implementation terms, the ideas in this paper are quite new (compare
USENIX McKusick). But also, Dodder performance is now much closer to
hand-tuned system performance for tar and rm and we are working towards
improving Postmark.
 
> 
> Other comments
> 
> 
> I got lost a few times in section 3, where the abstraction is defined
> 
>   o second para p 3 "every patch in p.ddeps should be committed to
>     disk either before, or at the same time as, p itself." This is
>     confusing -- I would have expected the requirement to be "every
>     patch in p.ddeps should be committed to disk before p itself."

This reviewer seems to be confused about the distinction between "<" and
"<=" as applied to the partial order on patches implied by the
dependencies. We will try to make this more clear to avoid this issue.

>     I suspect that the "at the same time as" clause only comes into
>     play when two patches write the same block? (Otherwise I can't
>     see how "at the same time as" can be realized...)

The reviewer is correct. We will try to make this more obvious.

> 
>   o \paragraph{Rollback data}: "A series of file system operations may
>     create dependencies that enforce a circular order among blocks,
>     even though the dependencies do not form a cycle [10]". It's been
>     a while since I read the soft updates paper, and this issue is not
>     ringing a bell. Unfortunately, a lot (most?) of the complexity of
>     the design seems to stem from this issue, so if a reader misses
>     this point, it is hard to appreciate/understand the rest of the
>     design. (Don't worry -- I'll go back and review soft updates so
>     that hopefully I can understand things better before I finish the
>     review, but any revision should explain this issue more since
>     otherwise you could lose a lot of readers.) [Later Figure 3 gives
>     an example of a cycle p1~->qh~->ph...ah, I think I get it now --
>     you can have multiple oustanding updates to the same block so that
>     the individual *updates* don't form a cycle, but there is a cycle
>     across *blocks*...OK I finally see what you're doing -- you only
>     keep one copy of the block in memory and apply writes to it; all
>     patches for the same blockID refer to the same block --> if you
>     have multiple outstanding writes, then the version of the block in
>     memory includes the effect of all outstanding writes. Now I
>     understand the purpose of the rollback inf...

See {Soft Updates Explanation}. We plan to spend some amount of more space
to (re)introduce soft updates concepts.

> 
>   o Near the end of page 3, it is not immediately obvious why
>     Dep[Flight[b]] can/should include Flight[b]. I'm guessing this is
>     handling the case of multiple outstanding patches to the same
>     block?

The reviewer is correct. We will try to clarify this.

> 
> Section 5 describes how this abstraction and architecture allow me to
> drop "soft updates" or "journaling" or "async writes" under the same
> file system written with explicit dependencies. I don't find any of
> these examples particularly compelling (nor do I find this flexibility
> particularly compelling.) I'll grant that it is "elegant" to be able
> to do this, and this is a contribution. At the same time, doing any of
> these things in a file system is well understood, and as a practical
> matter I already have the ability to pick and choose between soft
> updates, journaling, and async writes by choosing among different
> existing file systems. And this extra elegance and (perhaps)
> flexibility is not free -- we need to re-write our file systems and
> accept a performance hit. 

We somewhat agree with this reviewer's view. See {Motivation}. We plan
to change the reason why instant journaling is an interesting
contribution, from being a goal of the work to being an example
benefit of patches.

> 
> Two comments on the subversion example (para 2 page 11) 
> 
>   o Subversion maintainers disabled sync at the client because it was
>     "unacceptably slow". How does your implementation (with its
>     slowdowns) compare to this known "unacceptably slow" option?

The subversion maintainers never implemented an "unacceptably slow"
version of the subversion working copy library as far as we know.
Because subversion has always used its current consistency approach,
we do not think it would be interesting to include this as a
comparison point. Also see {Patchgroup Evaluation}. We plan to compare
Dodder and Linux subversion performance.

> 
>   o We know how to fix the problem -- use a jornaling ordered/full
>     data mode file system like ext3. Not clear that moving to patches
>     is easier or better than this. 

We plan to add performance measurements (see {Patchgroup Evaluation}).
The goal of this case study was to see how a large application with
an existing consistency protocol could be adapted to take advantage
of patchgroups. In subversion's case, the benefit is the client
has less strict file system operation ordering requirements and thus
is safe on more systems and, perhaps, more efficient on already-safe
systems. We believe a working copy library written from the start to
use patchgroups would benefit even more, but we do not feel such
a case study is in the scope of this paper.

> 
> The second version of exokernel (SOSP 97) tracked "tainted" blocks to
> enforce dependencies and included several optimizations to make this
> efficient. How does this approach differ?

We plan to add this comparison; thank you.
XN tracks block pointer level dependencies instead of the more flexible
and lower level dependencies in Dodder. XN's "temporary" file systems
are independent of Dodder. XN's non-referenced dependency optimization
would be useful in Dodder and we are in fact planning to introduce
a similar optimization.

> 
> Several points hint that work could increase with the number of
> outstanding requests.  I would like to see some high-throughput
> experiments (e.g., lots of concurrent writes to a RAID).

We plan to try to fit higher throughput experiments into the evaluation,
but are not sure there will be sufficient space for the additional value.

> 
> ==+== End Review
> 
> ==+== =====================================================================
> ==+== Begin Review #169E
> ==-== Paper: Generalized File System Dependencies
> ==-== Updated Tuesday 15 May 2007 2:21:56am EDT
> 
> ==+== A. Overall merit
> 
> 2. Weak reject
> 
> ==+== B. Novelty
> 
> 4. Novel
> 
> ==+== C. Writing
> 
> 3. Average
> 
> ==+== D. Reviewer expertise
> 
> 2. Some familiarity
> 
> ==+== E. Paper summary
> 
> The paper describes a new framework for writing file systems. The
> framework provides a "patch" abstraction, representing updates to
> blocks, and explicitly tracks dependencies among patches to order
> writes. The framework also provides a system for constructing file
> systems from modules, like VFS but more granular. The framework
> provides APIs that applications can use to express their consistency
> requirements in terms of dependencies. The framework has been
> implemented in Linux and the paper provides some evaluation.
> 
> ==+== F. Comments for author
> 
> The work described here is promising but it is not yet ready for
> publication. The big weakness of the paper is the evaluation section -
> it could be a lot clearer and more honest. And the results themselves
> are awful. Bottom line is a 2-3X performance penalty - that's just too
> much. I advise the authors to fix the perf bugs and revise the paper.

We agree that the evaluation section is the most problematic section in
the paper (see {Evaluation} above). We plan to rewrite this section and
add more evaluation. Dodder performance has also increased, and we plan
to further increase it.

> 
> Section 1 -
> file system correctness [8, 28] - what about citing the recent work
> from Dawson Engler's group?

We plan to add this reference.

> 
> "that which"

We plan to make this grammar improvement.

> 
> Section 3 -
> It wasn't clear at the beginning of section 3 that a patch was
> updating a byte-range inside the block instead of the entire block. I
> didn't realize this until 3.2. Please make this clear earlier.

We agree and plan to clarify this.

> 
> Rollback data - An example would really help.

See {Soft Updates Evaluation}. We plan to clarify rollback data.

> 
> Figure 3 - So the p patches are to one block and the q patches to a
> different block? Say this explicitly.

We plan to clarify.

> 
> Section 6 - "are has"

We plan to take this grammar fix.

> 
> "when the function returns, the patch *p must be
> set" - I'm not understanding this. So argument p is an in/out
> argument. I understand the input part, not the output.

We plan to clarify this patch interface convention explanation.
(Also mentioned in review A.)

> 
> Section 7.1 - I would like a better explanation of engaging &
> disengaging patchgroups. How does this work with threads? Is engaging
> a patchgroup the only way to actually use it to hold dependencies?
> How does this relate to the requirement in section 4.1 about knowing a
> patch's dependencies when the patch is created?

See {Patchgroup Interface}. We plan to clarify these patchgroup interface
questions (note that patchgroups cannot hold changes, by design) and
how patchgroups work.

> 
> Section 9 -
> 
> I don't see the value of comparing Dodder to FreeBSD. It's a different
> operating system with a different file system. The really interesting
> and fair comparison is Dodder to Linux/ext2, because the OS and the
> file system structures are the same.

We somewhat agree. We plan to remove the FreeBSD entry from the comparison
table, but do feel it is interesting enough to include inline with the text
because it provides a soft update file system comparison point.

> 
> In that comparison, the Dodder results are terrible (see figures 10,
> 11, 12) - Dodder is 2-3X worse. You need to fix this for Dodder to be
> interesting.

See {Performance}. Dodder is now similar to Linux performance for this
figure's measurements and we are improving other performance aspects.

> 
> I don't understand the division between 9.2 and 9.3. The untar and rm
> tests are not microbenchmarks. Aren't there any appropriate "standard"
> benchmarks, like the old "modified Andrew" benchmark?

The 9.2 and 9.3 division is incorrect and we agree that untar and rm
are not microbenchmarks, we plan to fix this. We also plan to add
one or two more standard benchmarks as part of {Evaluation}.

> 
> "suboptimal cache eviction, as described above" - I assume you mean
> the reference at the end of section 6.1. I think it would be better to
> move the discussion of cache performance to here to avoid the
> references back & forth. Better still if you instrumented enough to
> really understand instead of guess, better still if you fixed the
> problem :-).

We elected to address this point through its last route, fixing the
problem. We have actually found poor block allocation to have caused
much of Dodder ext2's slowness when compared with Linux ext2 and ext3.
Not cache eviction as we posited. {Performance} has more details.

> 
> Figure 11 - provide system time as well as real time

We plan to do this. Part of {Evaluation}.

> 
> Figure 13 - Measuring IMAP is great. Comparing to FreeBSD is not
> interesting, please compare to Linux/ext2 instead. Number of blocks is
> OK but better to measure number of IO requests, overall time, system
> time.

We plan do this. See {Patchgroup Evaluation}.

> 
> Section 9.4 - Summarize/interpret the results from the figure.

We plan to do this.

> 
> Reference 5 - Where was this thesis done?

We plan to add this information; it was done at the University
of Wisconsin-Madison.

> 
> Reference 14 - I couldn't find this TR. Please provide better citation.

We plan to include a URL. (Network Appliance removed their copy of the tech
report long ago. We plan to include an archive.org URL.)

> 
> ==+== End Review
> 
> ==+== =====================================================================
> ==+== Begin Review #169F
> ==-== Paper: Generalized File System Dependencies
> ==-== Updated Friday 8 Jun 2007 4:01:47pm EDT
> 
> ==+== A. Overall merit
> 
> 4. Accept
> 
> ==+== B. Novelty
> 
> 3. Incremental
> 
> ==+== C. Writing
> 
> 3. Average
> 
> ==+== D. Reviewer expertise
> 
> 3. Knowledgeable
> 
> ==+== E. Paper summary
> 
> The Doddler system is described which allows the kernel file system implementation and user space applications to explicitly declare the dependencies among writes to stable storage, i.e., the disks. Machinery in the kernel that is layered between the file system and the disk driver interprets these dependencies and assures that the on-disk state always includes all dependencies for any stable block. The file system implementation uses a variant of the dependency machinery that applies to disk blocks. Applications use a variant that applies to groups of file writes. An Linux kernel implementation is presented and the performance and correctness validated with various micro and macro benchmarks. The bottom line is that there are significant performance problems, but it may be possible to mitigate them by further clever optimizations. The explicit specification of the dependencies does seem to be a nice way to assure that the on disk data structures of both the file system and various applications are consistent.

> 
> ==+== F. Comments for author
> 
> This is a fascinating system. I think the idea has potential as a good way to explicitly handle the dependencies for on-disk data to make the stable state of a computer system consistent. I have several significant issues with the write up, however.
>    1) Until your got to "patch groups" on page 10 out of 14 I thought you were describing an SDK for writing various file and database system that operate directly on top of the disk. Since not many of these get written, I wasn't too interested. But patcg groups carries the ideas to the applications programmer, and suddenly it become much more useful in everyday programming. You need to describe this up from in the paper rather than bury it on page 10.

We plan to increase the paper's emphasis on patchgroups. Patchgroups
are already mentioned in the abstract and introduction, but we will
consider whether it would be good to introduce more of their meat earlier
in the paper.

> 
>    2) Patches contain the new values for a byte range in a disk block. You never tell us how the rest of the needed block is fetched and combined with the new values to form a complete new block to write back to disk.

Blocks are either completely read from disk (i.e. the block has existing
data) or patches initialize the entire block (i.e. the block has no existing
data) before (subsequent) patches are applied. We will try to clarify this
point.

> 
>    3) The discussion in section 3 under "Rollback data" is completely mysterious. I understand that when the dependencies are actually between objects within pages (e.g., I-nodes and directory descriptors) and not whole pages, then you can get false circular dependencies among the dirty pages containing these objects. I don't understand how the rollback data nor the algorithm vaguely described in this paragraph fixes the problem. And this has a huge impact on the credibility of the rest of the paper, as many of your performance optimizations are required exactly because the rollback data is voluminous and frequently unnecessary. You've got to make this all clearer. (I think it has something to do with the answer to      point 2.

We think this a misunderstanding or lack of understanding of soft updates
and hope to address it by expanding our soft updates review.
See {Soft Updates Explanation}.

> 
>    4) The discussion to related work is confusing. At the end summarize how the work improves the state of the art, now that the reader has been reminded of the previous work.

This is a good suggestion. We plan to expand comparisons with existing
systems.

> 
>    5) You need to remind the read of the key point surrounding soft updates.

We plan to do this. See {Soft Updates Explanation}.

> 
> Do these ideas carry over into the realm of distributed data storage. Seems like they should and perhaps you should look into it. 
> .

We think patches in distributed data storage is beyond this paper's scope,
but we have in fact thought about it on a few occasions. We may follow up
on this in the future.

> 
> ==+== End Review
> 
> ==+== =====================================================================
> ==+== Begin Review #169G
> ==-== Paper: Generalized File System Dependencies
> ==-== Updated Monday 11 Jun 2007 3:47:53pm EDT
> 
> ==+== A. Overall merit
> 
> 4. Accept
> 
> ==+== B. Novelty
> 
> 4. Novel
> 
> ==+== C. Writing
> 
> 4. Good
> 
> ==+== D. Reviewer expertise
> 
> 2. Some familiarity
> 
> ==+== E. Paper summary
> 
> This paper presents a way of describing dependencies between disk blocks to a buffer cache so that cache writeback policies can enforce them.  The work generalizes the "write before" relationship that is present in many storage systems.  The motivation, design, and implementation of the system is describe including a few case studies of applications using the mechanism and benchmarks comparing the performance against existing system.

The second sentence here is a very good one-sentence summary of our
work; it is perhaps more concise and motivating than any of our attempts
to say the same thing in the abstract and introduction. We will probably
incorporate a sentence like this into the final version of the paper.

> ==+== F. Comments for author
> 
> This paper is well written and does a good job of motivating, describing, and evaluating the work. The overall idea of formalizing the block cache writeback policy and giving control to application seems like a good one. 
> 
> One problem with the idea is the evaluation section seems to point out that the idea has a rather high performance cost even with all the optimizations in place. With this large of performance difference between Dobber and the "hand coded" dependencies of journaling file systems, it seems unlikely to be adopted.

See {Performance}. We are addressing Dodder's performance.

> 
> ==+== End Review


==+== Primary todos

From the above, our main TODO items (those with many people who commented
and/or we believe will involve substantial work) are as follows:
* Update and better express Dodder's motivation
* Better explain soft update ideas (primarily, block level cycles and related)
* Make patch optimizations clearer (including more examples?)
* Add more details to patchgroups
* Rewrite evaluation section
* Improve Dodder performance


==+== Schedule

We plan to give you a draft near final, but with more performance
improvements to go, the week before the final copy deadline.
TODO: more.
