==+== sosp2007 Paper Reviews

==+== =====================================================================
==+== Begin Review #169A
==-== Paper: Generalized File System Dependencies
==-== Updated Monday 23 Apr 2007 1:49:25pm EDT

==+== A. Overall merit

3. Weak accept

==+== B. Novelty

4. Novel

==+== C. Writing

4. Good

==+== D. Reviewer expertise

2. Some familiarity

==+== E. Paper summary

The paper proposes a generalized abstraction, called "patches", for
describing ordering constrainst among modifications to stable storage,
and shows how it can be used in file systems to replace
system-specific implementation for ensuring consistency in the
presence of failures. Patches directly describe the dependencies
between writes of disk blocks (in the file system context). They can
be used to implement soft updates, journaling, and other consistency
models that typically have special purpose implementations. A naive
implementation of patches has very high overhead in terms of CPU and
memory usage. The authors describe several optimizations to patches
that improves performance significantly and puts the idea in the realm
of the feasible. Their performance results show that there is still a
ways to go to get patches to perform as well as existing Linux systems
that use special-case consistency implementations.

==+== F. Comments for author

In general I like the paper. I think the idea is interesting, and you
give a balanced presentation of its current pros and cons. This seems
like an attempt at generalization that is may be worthwhile (unlike
so many others). If you can achieve the performance improvements that
you are aiming for it sounds like this could become a practical
abstraction for implementing stable storage systems.

Here are some specific comments:

You use the phrase "soft updates-like consistency" several
times. Aside from being a bit awkward, you don't define what it means
until well after the first use. I think I first noticed it on page 3
("Rollback data" subsection).

In the Rollback data section, I found the discussion about circular
dependencies a little hard to follow. May an example would help?

In Section 6, at the top of page 9, you write "When the function is
called, the patch *p is set to the patch, if any, on which the
modification should depend; when the function returns, the patch *p
must be set to depend on the modification itself." Did you mean that
*p must be set to the patch corresponding to the modification? (I
don't understand what it means otherwise.)

In section 7.1 on the patchgroups interface, you describe restrictions
to prevent cycles. So, can you create patchgroups p and q, then issue
pg_depend(q,p), and then engage each of p and q? (Is the rule just
that they can't be engaged before the "depend" call?) I found that a
little confusing on first read.

Can patches in a patchgroup be sent to disk before a patchgroup is
closed? Does closing a patchgroup do anything special?

In the description of using patchgroups in Subversion, I think Figure
8 would be more useful if you could tag the various components and
dependencies in the figure and refer to them in the discussion.

Finally, in Section 8, in describing the implementation, it would help
readers who are not Linux kernel experts (like me) to understand better how
Dodder fits in if you could give a little more context about how the
Linux kernel is structured around the buffer cache and file system
layers.

==+== End Review

==+== =====================================================================
==+== Begin Review #169B
==-== Paper: Generalized File System Dependencies
==-== Updated Monday 23 Apr 2007 2:15:16pm EDT

==+== A. Overall merit

4. Accept

==+== B. Novelty

4. Novel

==+== C. Writing

4. Good

==+== D. Reviewer expertise

4. Expert

==+== E. Paper summary

This paper describes a general abstraction, patches, for representing
write ordering in file systems.  The motivation is that by explicitly
labeling dependencies, one can be sure that updates are performed
consistently.  The authors describe a number of essential
optimizations for reducing the number of patches and rollbacks.  The
authors implement soft updates, journaling, and asynchronous writes
(no dependencies) with Dodder (though they do not show performance for
journaling). The authors also describe how applications can use
patchgroups to specify dependencies.

I like this paper and think that it makes interesting contributions.
First, the authors have implemented a general framework in Linux for
defining write dependencies; this framework is used to specify
consistency protocols such as soft updates and journaling.  Second,
the authors describe (and briefly evaluate) optimizations for reducing
the needed number of patches and rollbacks; these optimizations are
technically and conceptually interesting and non-trivial.

The serious problem with this work is that the performance of Dodder
is many times worse than that of ext2 or ext3.  The authors state that
their overall performance is "competitive" (abstract) and "within
reason" (Section 9), but I don't agree.  Their current
numbers (for Postmark) indicate that Dodder requires 47s, compared to
17s and 20s, for ext2 and ext3 respectively; their results for
microbenchmarks of untar and delete and similarly worrisome.  

Another weakness is that the authors have not performed a
thorough evaluation.  The best way to compare performance would be to
implement soft updates directly in ext2 and with Dodder, and compare
the two; the authors do not do this.  Additionally, the authors should
compare ext3 to Dodder with journaling, but they show no numbers for
Dodder journaling.

I am also disappointed with the evaulation of the case studies using
patchgroups.  The gzip example could be solved with an fsync and the
subversion example already provides safe commits with syncs on the
server and journaling on the client; the performance of the non-patch
versions is in no way compared to patches.  The IMAP example, which
originally used syncs, is more compelling.  This is the only example
that the authors show any performance results for; however, their
metric is # of writes, and not performance in time.  It concerns me
that the authors are hiding poor performance with their indirect
metric.  I would like to see performance results for all three case
studies.

My final serious concern is that the motivation for patches is not
strong.  Is a general abstraction really needed?  Where are the
benefits?  (Certainly it is not a performance benefit.)  The authors
mention that patches give file system implementations some flexibility
in how dependencies are enforced, but they do not explore this
flexibility - they don't have results with multiple implementations.


I would like to see this paper accepted, under the assumption that the
authors are able to improve the performance of Dodder and more
thoroughly evaluate Dodder.

==+== F. Comments for author

As stated above, I think the most needed improvement for this paper is
to improve the performance and the evaluation of your system.

- Related Work: I'd like to see more comparison with Burnett's thesis.  Is Burnett's
  approach only for applications?  How does his interface differ from
  patches?

- Much of the text would be easier to follow if you included more
  examples (though I understand that you are space constrained right
  now).  For example, a case illustrating how the file system can
  create circular dependencies, even when the data itself does not (pg
  3).  I could have used more explanation for when hard patches cannot
  be applied; specifically, Figure 2 was useful for showing when the
  optimizations could be applied, but I could use an example where
  they cannot be applied too.  It would be useful to see the patches
  that result from more file and directory operations for soft
  updates.  For Figure 3, could you provide an example that represents
  specific updates?

- The authors consider the approach of carrying rollback data as a
  part of a patch, in case there is a circular dependency.  It seems
  another approach to breaking the dependency would be to create a
  copy of the page causing the circular dependency; is this
  equivalent?  how does this approach compare?

- Figures 4 and 5: I would have preferred a diagram that shows the file
  system data structures being manipulated (similar to Figure 2)
  instead.

- Section 5, Journaling: It took me awhile to figure out what you
  meant by your last paragraph.  I think the problem is that this
  discussion assumes knowledge of the Modules, not described until
  Section 6.  I'd also say that your model is similar to ext3's *data*
  journaling mode (if that is indeed the case, and not the default
  ordered journaling mode).

- Figure 6: The font for the interface specifications is too small
  (CFS, L2FS, BD).  

- I'd be interested in seeing the experiments exploring the
  sensitivity to the write-back cache policy for dirty blocks.  It
  certainly seems that performance could be improved by waiting until
  more patches are specified before flushing.

- As stated above, I would like to see more detailed analysis of
  patchgroups.  First, I'd like to see all three applications
  analyzed.  Second, the metric should be a direct measurement of
  performance, and not simply of the number of writes.  Third, I'd
  like an explanation for the performance differences and comments on
  any difference in ordering guarantees across the versions.

- You might want to consider how you could express shadow paging, as
  in WAFL, with patches.  Being able to demonstrate an additional
  consistency mechnanism would help convince readers of the generality
  of patches.  I believe this should be straight forward.

==+== End Review

==+== =====================================================================
==+== Begin Review #169C
==-== Paper: Generalized File System Dependencies
==-== Updated Tuesday 24 Apr 2007 9:56:48pm EDT

==+== A. Overall merit

3. Weak accept

==+== B. Novelty

3. Incremental

==+== C. Writing

4. Good

==+== D. Reviewer expertise

2. Some familiarity

==+== E. Paper summary

This paper proposes a general patch abstraction that can capture any
write dependencies in a file system. The authors demonstrate the
generality and practicality of their approach by showing how several
file applications and systems, such as gzip and svn, can be
implemented on top of the patch abstraction. The paper presents
several optimizations that dramatically reduce the overhead of their
patching implementation.

==+== F. Comments for author

This is a solid paper. My comments are below.

While in general I like the paper, I am not sure whether the decision
to add the patch functionality at the lower layers of the storage
stack is the right one. The decision of how much functionality to add
at a lower layer is often a hard one. In the networking world, this
decision is typically resolved by invoking the end-to-end arguments
principle. In one of its interpretations, this principle says that it
is fine to add functionality at a lower layer as long as this
functionality can dramatically improve the performance of some
important applications, and, at the same time, does not negatively
impact the applications that do not use the new functionality. I am
not sure that the proposed solution passes this threshold. A more
comprehensive discussion of this decision would help.

Section 7.2 does a good job of evaluating the solution by showing that
implementing it requires small changes to existing applications and
the overhead is "reasonable". One thing that is missing though is
quantifying the savings due to the better control of dependencies
enabled by the patch abstraction. In both the case of the subversion
example and the UW IMPA example, using the patch abstraction allows
one to relax the dependencies imposed by the original
application. Intuitively, this should have a positive effect on
performance. It is true that in the end the overhead of implementing
patching may offset these gains, but it would be still interesting to
quantify the potential gains.

Other comments and typos:

- page 1, 2nd column, 3rd para: "optimizations that which" ->
  "optimizations that"

- page 4, 2nd column, 4th para: "On the one hand" -> "On one hand"

- page 4: You say that your implementation "currently disallows any
  dependency cycles..." What is the impact of this constraint, if any?

- page 5: You should explain better Figure 2(a). It took me quite a
  bit of time to understand what each patch represents.

- page 8, "Soft updates": While in the end it is clear from the text
  what kind of consistency you are referring to, it would be nice to
  avoid vague formulations such as "relative consistency" and "more
  serious inconsistencies"

- page 8, sec 6: "There are has" -> "There are"

- page 10, 2nd column, para before Sec 7.2: "When an patchgroup" ->
  "When a patchgroup"

- page 10, "Gzip": Remove one occurrence of "trivial" from the first
  sentence.

==+== End Review

==+== =====================================================================
==+== Begin Review #169D
==-== Paper: Generalized File System Dependencies
==-== Updated Monday 14 May 2007 10:24:13pm EDT

==+== A. Overall merit

2. Weak reject

==+== B. Novelty

2. Very incremental

==+== C. Writing

3. Average

==+== D. Reviewer expertise

3. Knowledgeable

==+== E. Paper summary

the paper argues for explicitly specifying which disk writes depend on
which others. Specification is done on a fine-grained basis (e.g.,
each updated field in an inode might be specified as a separate write)
and the run-time system merges writes to keep things reasonably
efficient. The runtime system then ensures that a write doesn't make
it to disk before the blocks on which it depends. 

Claimed benefits

  o It is A Good Thing to specify dependencies explicitly

  o Simplifies constructing file systems

  o User-level processes can specify dependencies that are correctly
    propagated through the system's layers

  o The optimizations keep the overheads of this approach managable


Limitations

  o The motivation/significance of this work seems limited

  o This paper primarily cleans up and simplifies ideas from the soft
    updates paper rather than introducing a fundamentally new approach.

  o Most of the writing is quite clear, but a few crucial points
   (regarding cycles and rollback) were not made clear to me even
   after several careful readings

  o The performance hit is currently non-trivial, and I'm not sure
    that it will be an easy sell to get people to pay the cost of
    significantly worse performance in order to get the benefit of "a
    cleaner abstraction"

Summary recommendation

  Although I half buy the philosophy that explicitly specifying
  dependencies seems like a good way to ensure dependencies are
  respected, the practical impact seems marginal. Also, the basic
  ideas don't seem that different than the soft updates paper (as the
  title promises, this paper "generalizes" file system dependencies,
  it doesn't introduce the idea.) Worse, this generalization ends up
  coming at a pretty high performance cost.  As a result, I find it
  hard to argue for accepting the paper. My rating of "weak reject"
  reflects that I have positive feelings about the paper, but that I
  don't think it is in the top half of the (generally very good to
  excellent) papers I reviewed.

==+== F. Comments for author

Two high level points then a few detailed comments.

High level question -- is this the right abstraction? An
alternative would be to expose a transactional interface.  Sure, you
lose the ability to switch between "soft updates" and "logging" styles
of reliability, but (a) I'm not entirely confinced this is a big loss
and (b) you may end up with simpler specifications of systems above
this layer. 


As the authors make clear in the title, this paper generalizes ideas
from the 1999 USENIX soft updates paper. In particular, the basic
ideas of "track dependencies at fine grained basis" and "roll back to
fix cycles" were both in the 1999 paper ("The key attribute of soft
updates is dependency tracking at the level of individual changes
within cached blocks.... With this level of detailed dependency
information, circular dependencies between blocks are not problematic.
For example, when the system wishes to write a buffer containing
inodes, those inodes that can be safely written can go to the
disk. Any inodes that cannot be safely written yet are temporarily
rolled back to their safe values while the disk write proceeds. After
the disk write completes, such inodes are rolled forward to their
current values.")

Refining our understanding of existing ideas is a laudible goal. The
paper would be stronger if the ideas can be refined to the point where
the costs of the general system are comparable to hand-tuned
systems. (That will be even more convincing information that the work
has a complete "deep" understanding of the issues.)



Other comments


I got lost a few times in section 3, where the abstraction is defined

  o second para p 3 "every patch in p.ddeps should be committed to
    disk either before, or at the same time as, p itself." This is
    confusing -- I would have expected the requirement to be "every
    patch in p.ddeps should be committed to disk before p itself." I
    suspect that the "at the same time as" clause only comes into play
    when two patches write the same block? (Otherwise I can't see how
    "at the same time as" can be realized...)

  o \paragraph{Rollback data}: "A series of file system operations may
    create dependencies that enforce a circular order among blocks,
    even though the dependencies do not form a cycle [10]". It's been
    a while since I read the soft updates paper, and this issue is not
    ringing a bell. Unfortunately, a lot (most?) of the complexity of
    the design seems to stem from this issue, so if a reader misses
    this point, it is hard to appreciate/understand the rest of the
    design. (Don't worry -- I'll go back and review soft updates so
    that hopefully I can understand things better before I finish the
    review, but any revision should explain this issue more since
    otherwise you could lose a lot of readers.) [Later Figure 3 gives
    an example of a cycle p1~->qh~->ph...ah, I think I get it now --
    you can have multiple oustanding updates to the same block so that
    the individual *updates* don't form a cycle, but there is a cycle
    across *blocks*...OK I finally see what you're doing -- you only
    keep one copy of the block in memory and apply writes to it; all
    patches for the same blockID refer to the same block --> if you
    have multiple outstanding writes, then the version of the block in
    memory includes the effect of all outstanding writes. Now I
    understand the purpose of the rollback inf...

  o Near the end of page 3, it is not immediately obvious why
    Dep[Flight[b]] can/should include Flight[b]. I'm guessing this is
    handling the case of multiple outstanding patches to the same
    block?

Section 5 describes how this abstraction and architecture allow me to
drop "soft updates" or "journaling" or "async writes" under the same
file system written with explicit dependencies. I don't find any of
these examples particularly compelling (nor do I find this flexibility
particularly compelling.) I'll grant that it is "elegant" to be able
to do this, and this is a contribution. At the same time, doing any of
these things in a file system is well understood, and as a practical
matter I already have the ability to pick and choose between soft
updates, journaling, and async writes by choosing among different
existing file systems. And this extra elegance and (perhaps)
flexibility is not free -- we need to re-write our file systems and
accept a performance hit. 

Two comments on the subversion example (para 2 page 11) 

  o Subversion maintainers disabled sync at the client because it was
    "unacceptably slow". How does your implementation (with its
    slowdowns) compare to this known "unacceptably slow" option?

  o We know how to fix the problem -- use a jornaling ordered/full
    data mode file system like ext3. Not clear that moving to patches
    is easier or better than this. 



The second version of exokernel (SOSP 97) tracked "tainted" blocks to
enforce dependencies and included several optimizations to make this
efficient. How does this approach differ?


Several points hint that work could increase with the number of
outstanding requests.  I would like to see some high-throughput
experiments (e.g., lots of concurrent writes to a RAID).

==+== End Review

==+== =====================================================================
==+== Begin Review #169E
==-== Paper: Generalized File System Dependencies
==-== Updated Tuesday 15 May 2007 2:21:56am EDT

==+== A. Overall merit

2. Weak reject

==+== B. Novelty

4. Novel

==+== C. Writing

3. Average

==+== D. Reviewer expertise

2. Some familiarity

==+== E. Paper summary

The paper describes a new framework for writing file systems. The
framework provides a "patch" abstraction, representing updates to
blocks, and explicitly tracks dependencies among patches to order
writes. The framework also provides a system for constructing file
systems from modules, like VFS but more granular. The framework
provides APIs that applications can use to express their consistency
requirements in terms of dependencies. The framework has been
implemented in Linux and the paper provides some evaluation.

==+== F. Comments for author

The work described here is promising but it is not yet ready for
publication. The big weakness of the paper is the evaluation section -
it could be a lot clearer and more honest. And the results themselves
are awful. Bottom line is a 2-3X performance penalty - that's just too
much. I advise the authors to fix the perf bugs and revise the paper.

Section 1 -
file system correctness [8, 28] - what about citing the recent work
from Dawson Engler's group?
"that which"

Section 3 -
It wasn't clear at the beginning of section 3 that a patch was
updating a byte-range inside the block instead of the entire block. I
didn't realize this until 3.2. Please make this clear earlier.
Rollback data - An example would really help.

Figure 3 - So the p patches are to one block and the q patches to a
different block? Say this explicitly.

Section 6 - "are has"
"when the function returns, the patch *p must be
set" - I'm not understanding this. So argument p is an in/out
argument. I understand the input part, not the output.

Section 7.1 - I would like a better explanation of engaging &
disengaging patchgroups. How does this work with threads? Is engaging
a patchgroup the only way to actually use it to hold dependencies?
How does this relate to the requirement in section 4.1 about knowing a
patch's dependencies when the patch is created?

Section 9 -

I don't see the value of comparing Dodder to FreeBSD. It's a different
operating system with a different file system. The really interesting
and fair comparison is Dodder to Linux/ext2, because the OS and the
file system structures are the same.

In that comparison, the Dodder results are terrible (see figures 10,
11, 12) - Dodder is 2-3X worse. You need to fix this for Dodder to be
interesting.

I don't understand the division between 9.2 and 9.3. The untar and rm
tests are not microbenchmarks. Aren't there any appropriate "standard"
benchmarks, like the old "modified Andrew" benchmark?

"suboptimal cache eviction, as described above" - I assume you mean
the reference at the end of section 6.1. I think it would be better to
move the discussion of cache performance to here to avoid the
references back & forth. Better still if you instrumented enough to
really understand instead of guess, better still if you fixed the
problem :-).

Figure 11 - provide system time as well as real time

Figure 13 - Measuring IMAP is great. Comparing to FreeBSD is not
interesting, please compare to Linux/ext2 instead. Number of blocks is
OK but better to measure number of IO requests, overall time, system
time.

Section 9.4 - Summarize/interpret the results from the figure.

Reference 5 - Where was this thesis done?

Reference 14 - I couldn't find this TR. Please provide better citation.

==+== End Review

==+== =====================================================================
==+== Begin Review #169F
==-== Paper: Generalized File System Dependencies
==-== Updated Friday 8 Jun 2007 4:01:47pm EDT

==+== A. Overall merit

4. Accept

==+== B. Novelty

3. Incremental

==+== C. Writing

3. Average

==+== D. Reviewer expertise

3. Knowledgeable

==+== E. Paper summary

The Doddler system is described which allows the kernel file system implementation and user space applications to explicitly declare the dependencies among writes to stable storage, i.e., the disks. Machinery in the kernel that is layered between the file system and the disk driver interprets these dependencies and assures that the on-disk state always includes all dependencies for any stable block. The file system implementation uses a variant of the dependency machinery that applies to disk blocks. Applications use a variant that applies to groups of file writes. An Linux kernel implementation is presented and the performance and correctness validated with various micro and macro benchmarks. The bottom line is that there are significant performance problems, but it may be possible to mitigate them by further clever optimizations. The explicit specification of the dependencies does seem to be a nice way to assure that the on disk data structures of both the file system and various applications are consistent.

==+== F. Comments for author

This is a fascinating system. I think the idea has potential as a good way to explicitly handle the dependencies for on-disk data to make the stable state of a computer system consistent. I have several significant issues with the write up, however.
   1) Until your got to "patch groups" on page 10 out of 14 I thought you were describing an SDK for writing various file and database system that operate directly on top of the disk. Since not many of these get written, I wasn't too interested. But patcg groups carries the ideas to the applications programmer, and suddenly it become much more useful in everyday programming. You need to describe this up from in the paper rather than bury it on page 10.
   2) Patches contain the new values for a byte range in a disk block. You never tell us how the rest of the needed block is fetched and combined with the new values to form a complete new block to write back to disk.
   3) The discussion in section 3 under "Rollback data" is completely mysterious. I understand that when the dependencies are actually between objects within pages (e.g., I-nodes and directory descriptors) and not whole pages, then you can get false circular dependencies among the dirty pages containing these objects. I don't understand how the rollback data nor the algorithm vaguely described in this paragraph fixes the problem. And this has a huge impact on the credibility of the rest of the paper, as many of your performance optimizations are required exactly because the rollback data is voluminous and frequently unnecessary. You've got to make this all clearer. (I think it has something to do with the answer to      point 2.
   4) The discussion to related work is confusing. At the end summarize how the work improves the state of the art, now that the reader has been reminded of the previous work.
   5) You need to remind the read of the key point surrounding soft updates.

Do these ideas carry over into the realm of distributed data storage. Seems like they should and perhaps you should look into it. 
.

==+== End Review

==+== =====================================================================
==+== Begin Review #169G
==-== Paper: Generalized File System Dependencies
==-== Updated Monday 11 Jun 2007 3:47:53pm EDT

==+== A. Overall merit

4. Accept

==+== B. Novelty

4. Novel

==+== C. Writing

4. Good

==+== D. Reviewer expertise

2. Some familiarity

==+== E. Paper summary

This paper presents a way of describing dependencies between disk blocks to a buffer cache so that cache writeback policies can enforce them.  The work generalizes the $B!H(Bwrite before$B!I(B relationship that is present in many storage systems.  The motivation, design, and implementation of the system is describe including a few case studies of applications using the mechanism and benchmarks comparing the performance against existing system.

==+== F. Comments for author

This paper is well written and does a good job of motivating, describing, and evaluating the work. The overall idea of formalizing the block cache writeback policy and giving control to application seems like a good one. 

One problem with the idea is the evaluation section seems to point out that the idea has a rather high performance cost even with all the optimizations in place. With this large of performance difference between Dobber and the $B!H(Bhand coded$B!I(B dependencies of journaling file systems, it seems unlikely to be adopted.

==+== End Review
