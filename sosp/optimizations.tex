% -*- mode: latex; tex-main-file: "paper.tex" -*-

\section{\Patch\ Optimizations}
\label{sec:patch:optimizations}

\begin{comment}
%% This initial paragraph irritates me now.
A naive \Featherstitch\ implementation 
%% of this model has all the properties we want except good performance.
%
allows file system modules to define and modify consistency-preserving
orderings, and allows applications to add consistency requirements of their
own (using the interface described below).
%
But although all this is relatively easy, it performs badly.
\end{comment}


Figure~\ref{fig:opt}a shows the \patches\ generated by a naive \Kudos\
implementation when an application appends
16~kB of data to an existing empty file using four 4~kB writes.
%
The file system is ext2 with soft updates-like
dependencies and 4~kB blocks.
%
%Each circle represents a single \patch. The shaded boxes indicate disk blocks.
%
%The arrows between \patches\ represent the dependency relationship \PDDepend.
%
%
Four blocks are allocated (\patches\ $b_1$--$b_4$),
written ($d_1$--$d_4$ and $d_1'$--$d_4'$), and attached to the file's
inode ($i_1$--$i_4$); the inode's file size and modification time are updated
($i_1'$--$i_4'$ and $i''$); and changes to the ``group descriptor'' and superblock
account for the allocated blocks ($g$ and $s$).
%
\begin{comment}
The operation is broken into four one-block appends; the numeric subscripts in
the \patch\ labels indicate with which each \patch\ is associated.
\end{comment}
%
Each application write updates the inode; note, for example, how overlap
dependencies force each modification of the inode's size to depend on the
previous one.
%
A total of eight blocks are written during the operation.
%
Unoptimized \Kudos, however, represents the operation with 23 \patches\ and
roughly 33,000 (!) bytes of undo data.
%
The \patches\ slow down the buffer cache system by making graph traversals
more expensive.
%
Storing undo data for \patches\ on data blocks is particularly painful
here, since they will \emph{never} need to be reverted.
%
And in larger examples, the effects are even worse. For example, when
256~MB of blocks are allocated in the untar benchmark described in
Section~\ref{eval}, unoptimized \Kudos\ allocates an additional 533~MB,
mostly for \patches\ and undo data.

This section presents optimizations based on generic
dependency analysis that reduce these 23 \patches\ and 33,000 bytes
of undo data to the 8 \patches\ and 0 bytes of
undo data in Figure~\ref{fig:opt}d.
%
Additional optimizations simplify \Kudos's other main overhead, the CPU
time required for the buffer cache to find a suitable set of patches to
write.
%
These optimizations apply transparently to any \Kudos\ file system, and as
we demonstrate in our evaluation, have dramatic effects on real benchmarks
as well; for instance, they reduce memory overhead in the untar benchmark
from 533~MB to just 40~MB.

\begin{comment}

Challenges in a \patch-based file system implementation include:

\textbf{Buffer cache graph traversal.}
%
In order to evict and write a block, the buffer cache must choose a block
$b$,
%
and then find a set of \patches\ $P_b \subseteq \PMem[b]$ whose dependencies
satisfy a graph property, namely that $\PDepset{P_b} \subseteq P_b \cup
\PDisk$.
%
It usually makes sense to define $P_b$ maximally---that is, as the
\emph{largest} corresponding set of \patches.
%
In the ideal (and common) case $P_b = \PMem[b]$, which lets the cache reuse
$b$'s memory once $P_b$ is committed to disk.  However, in some cases there
may be no block for which $P_b = \PMem[b]$.
%
It would also be nice if the blocks chosen for writing also maximized the
disk's commit rate, by minimizing seeks and so forth.

A naive implementation might calculate, for each in-memory block $b$, the
largest set of \patches\ $P_b \subseteq \PMem[b]$ with $\PDepset{P_b}
\subseteq P_b \cup \PDisk$, then evict some block close to previously
written blocks and with few reverted \patches\ (where $\PMem[b] - P_b$
is small).
%
This, however, would be extraordinarily expensive.
%
Finding $P_b$ requires traversing a dependency graph which might contain
thousands and thousands of nodes.
%
Doing so for each block, once per eviction, would take huge amounts of CPU
time.


\textbf{Undo memory usage.}
%
Only a small fraction of \patches\ will ever need to be reverted.
%
For example, most data writes never need to be reverted in any file
system.
%
If a \patch\ won't be reverted under any circumstances, the memory and
CPU time spent to preserve the old version is wasted.


\textbf{\Patch\ memory usage.}
%
\Patches\ themselves take up memory and require time to allocate, free, and
traverse.
%
If two \patches\ have redundant dependencies, it would be faster to combine
them.


%% \textbf{Dependency memory usage.}
%% %
%% The $\PDDepset{}$ sets are stored as doubly linked lists; each individual
%% dependency takes up memory.
%% %
%% Important and common dependency relations require many dependencies to
%% express; for example, if \patches\ $p_1,\dots,p_n$ depend, as a group, on
%% \patches\ $q_1,\dots,q_n$, expressing this constraint would require $n^2$
%% total dependencies.


The next section tackles all of these challenges.
\end{comment}


\begin{figure}
\centering

\includegraphics[scale=0.62]{fig/opt_1}

\textbf{a)} Naive implementation

\vskip.5\baselineskip

\includegraphics[scale=0.62]{fig/opt_2}

\textbf{b)} With \nrb\ \patches\ \dots

\vskip.5\baselineskip

\includegraphics[scale=0.62]{fig/opt_3}

\textbf{c)} \dots plus \nrb\ \patch\ merging \dots

\vskip.5\baselineskip

\includegraphics[scale=0.62]{fig/opt_4}

\textbf{d)} \dots plus overlap merging

\caption{\Patches\ required to append 4 blocks to an existing file, without
and with optimization.  \Nrb\ \patches\ are shown with heavy borders.}
\label{fig:opt}
\label{f:opt} % EDDIE: OK
\end{figure}

\input{nrb}
\input{merge}
\input{readylist}
\input{misc}
